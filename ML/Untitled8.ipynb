{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbj-Hvi9A8T-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import signal\n",
        "import tensorflow as tf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "from collections import deque\n",
        "import threading\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'sampling_rate': 360,  # Hz - matches MIT-BIH standard\n",
        "    'window_size': 180,    # samples per beat (0.5 seconds)\n",
        "    'risk_window_minutes': 10,  # time window for risk calculation\n",
        "    'prediction_horizon_hours': 6,  # predict events within 6 hours\n",
        "}\n",
        "\n",
        "# Arrhythmia classes from MIT-BIH database\n",
        "CLASSES = {0: 'Normal', 1: 'SVE', 2: 'VEB', 3: 'Fusion', 4: 'Unknown'}\n",
        "\n",
        "# Clinical risk weights (higher = more dangerous)\n",
        "RISK_WEIGHTS = {'Normal': 0.0, 'SVE': 2.0, 'VEB': 5.0, 'Fusion': 8.0, 'Unknown': 3.0}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGPreprocessor:\n",
        "    def __init__(self, fs=360):\n",
        "        self.fs = fs\n",
        "        # Design filters for different noise types\n",
        "        self.hp_filter = signal.butter(4, 0.5/(fs/2), btype='high', output='sos')  # Remove baseline drift\n",
        "        self.lp_filter = signal.butter(4, 40/(fs/2), btype='low', output='sos')   # Remove muscle noise\n",
        "        self.notch_filter = signal.iirnotch(50, 30, fs, output='sos')             # Remove power line interference\n",
        "\n",
        "    def clean_signal(self, raw_ecg):\n",
        "        \"\"\"\n",
        "        Clean raw ECG signal by removing noise\n",
        "\n",
        "        Steps:\n",
        "        1. Remove DC offset (center signal around zero)\n",
        "        2. High-pass filter (remove breathing artifacts)\n",
        "        3. Notch filter (remove 50Hz electrical interference)\n",
        "        4. Low-pass filter (remove muscle artifacts)\n",
        "        5. Wavelet denoising (advanced mathematical smoothing)\n",
        "        \"\"\"\n",
        "        # Step 1: Remove DC offset\n",
        "        signal_clean = raw_ecg - np.mean(raw_ecg)\n",
        "\n",
        "        # Step 2: Remove baseline wander (breathing)\n",
        "        signal_clean = signal.sosfilt(self.hp_filter, signal_clean)\n",
        "\n",
        "        # Step 3: Remove power line interference\n",
        "        signal_clean = signal.sosfilt(self.notch_filter, signal_clean)\n",
        "\n",
        "        # Step 4: Remove high frequency noise\n",
        "        signal_clean = signal.sosfilt(self.lp_filter, signal_clean)\n",
        "\n",
        "        # Step 5: Wavelet denoising (removes remaining artifacts)\n",
        "        signal_clean = self._wavelet_denoise(signal_clean)\n",
        "\n",
        "        return signal_clean\n",
        "\n",
        "    def _wavelet_denoise(self, signal_data, threshold=0.04):\n",
        "        \"\"\"\n",
        "        Advanced denoising using Discrete Wavelet Transform\n",
        "        Think of this as mathematical noise reduction\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import pywt\n",
        "            # Decompose signal into frequency components\n",
        "            coeffs = pywt.wavedec(signal_data, 'sym8', level=6)\n",
        "\n",
        "            # Apply soft thresholding to remove noise\n",
        "            sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "            threshold_value = threshold * sigma\n",
        "\n",
        "            coeffs_clean = []\n",
        "            for i, coeff in enumerate(coeffs):\n",
        "                if i == 0:  # Keep low frequency components\n",
        "                    coeffs_clean.append(coeff)\n",
        "                else:  # Clean high frequency components\n",
        "                    coeffs_clean.append(pywt.threshold(coeff, threshold_value, mode='soft'))\n",
        "\n",
        "            # Reconstruct clean signal\n",
        "            return pywt.waverec(coeffs_clean, 'sym8')\n",
        "        except:\n",
        "            return signal_data  # Return original if wavelet not available"
      ],
      "metadata": {
        "id": "A-4kWcAjBxQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HeartbeatDetector:\n",
        "    def __init__(self, fs=360, window_size=180):\n",
        "        self.fs = fs\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "\n",
        "    def find_heartbeats(self, clean_ecg):\n",
        "        \"\"\"\n",
        "        Find individual heartbeats in ECG signal\n",
        "\n",
        "        Process:\n",
        "        1. Enhance QRS complexes (heartbeat spikes)\n",
        "        2. Find peaks with adaptive thresholding\n",
        "        3. Extract beat windows around each peak\n",
        "        4. Quality control - reject noisy beats\n",
        "        \"\"\"\n",
        "        # Step 1: Enhance QRS complexes using Pan-Tompkins algorithm\n",
        "        r_peaks = self._detect_r_peaks(clean_ecg)\n",
        "\n",
        "        # Step 2: Extract beat windows\n",
        "        beats, peak_locations = self._extract_beat_windows(clean_ecg, r_peaks)\n",
        "\n",
        "        return beats, peak_locations\n",
        "\n",
        "    def _detect_r_peaks(self, signal_data):\n",
        "        \"\"\"\n",
        "        Pan-Tompkins R-peak detection algorithm\n",
        "        Enhanced version of classic heartbeat detection\n",
        "        \"\"\"\n",
        "        # Differentiate signal (enhances sharp changes)\n",
        "        diff_signal = np.diff(signal_data)\n",
        "\n",
        "        # Square signal (emphasizes large changes)\n",
        "        squared = diff_signal ** 2\n",
        "\n",
        "        # Moving average (smooths the signal)\n",
        "        window_samples = int(0.08 * self.fs)  # 80ms window\n",
        "        smoothed = np.convolve(squared, np.ones(window_samples)/window_samples, mode='same')\n",
        "\n",
        "        # Find peaks with adaptive thresholding\n",
        "        peaks, _ = signal.find_peaks(\n",
        "            smoothed,\n",
        "            height=0.3 * np.max(smoothed),     # Must be 30% of max height\n",
        "            distance=int(0.6 * self.fs),      # Minimum 600ms between beats (100 BPM max)\n",
        "            prominence=0.1 * np.max(smoothed)  # Must stand out from surroundings\n",
        "        )\n",
        "\n",
        "        # Refine peak locations to original signal maxima\n",
        "        refined_peaks = []\n",
        "        for peak in peaks:\n",
        "            # Search Â±100ms around detected peak\n",
        "            search_start = max(0, peak - int(0.1 * self.fs))\n",
        "            search_end = min(len(signal_data), peak + int(0.1 * self.fs))\n",
        "\n",
        "            # Find actual maximum in original signal\n",
        "            local_segment = signal_data[search_start:search_end]\n",
        "            local_max_idx = np.argmax(local_segment)\n",
        "            actual_peak = search_start + local_max_idx\n",
        "\n",
        "            refined_peaks.append(actual_peak)\n",
        "\n",
        "        return np.array(refined_peaks)\n",
        "\n",
        "    def _extract_beat_windows(self, signal_data, r_peaks):\n",
        "        \"\"\"\n",
        "        Extract 180-sample windows around each R-peak\n",
        "        \"\"\"\n",
        "        beats = []\n",
        "        valid_peaks = []\n",
        "\n",
        "        for peak in r_peaks:\n",
        "            start_idx = peak - self.half_window\n",
        "            end_idx = peak + self.half_window\n",
        "\n",
        "            # Check if window is within signal bounds\n",
        "            if start_idx >= 0 and end_idx < len(signal_data):\n",
        "                beat_window = signal_data[start_idx:end_idx]\n",
        "\n",
        "                # Quality control - only keep good beats\n",
        "                if self._is_good_quality(beat_window):\n",
        "                    beats.append(beat_window)\n",
        "                    valid_peaks.append(peak)\n",
        "\n",
        "        return np.array(beats), np.array(valid_peaks)\n",
        "\n",
        "    def _is_good_quality(self, beat):\n",
        "        \"\"\"\n",
        "        Quality control: reject noisy or invalid beats\n",
        "        \"\"\"\n",
        "        # Check amplitude range (too flat = bad)\n",
        "        if np.max(beat) - np.min(beat) < 0.1:\n",
        "            return False\n",
        "\n",
        "        # Check for excessive high-frequency noise\n",
        "        noise_level = np.sum(np.abs(np.diff(beat, 2)))\n",
        "        if noise_level > 10 * np.std(beat):\n",
        "            return False\n",
        "\n",
        "        # Check for reasonable beat morphology\n",
        "        if np.any(np.isnan(beat)) or np.any(np.isinf(beat)):\n",
        "            return False\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "id": "wW_xF3Q8B1W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMFeatureExtractor:\n",
        "    def __init__(self, input_shape=(180, 1)):\n",
        "        self.input_shape = input_shape\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def build_network(self):\n",
        "        \"\"\"\n",
        "        Build BiLSTM network for feature extraction\n",
        "\n",
        "        Architecture:\n",
        "        - Input: 180 ECG samples per beat\n",
        "        - BiLSTM Layer 1: 128 units, bidirectional (looks forward and backward)\n",
        "        - BiLSTM Layer 2: 128 units, bidirectional\n",
        "        - Dense layers: Extract 7200 features\n",
        "        - Output: Both features and classification\n",
        "        \"\"\"\n",
        "        inputs = tf.keras.layers.Input(shape=self.input_shape)\n",
        "\n",
        "        # Bidirectional LSTM layers (analyze beat in both directions)\n",
        "        lstm1 = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)\n",
        "        )(inputs)\n",
        "\n",
        "        lstm2 = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(128, return_sequences=False, dropout=0.2)\n",
        "        )(lstm1)\n",
        "\n",
        "        # Feature extraction layers\n",
        "        dense1 = tf.keras.layers.Dense(512, activation='relu')(lstm2)\n",
        "        dropout1 = tf.keras.layers.Dropout(0.3)(dense1)\n",
        "\n",
        "        # Main feature output (7200 features as per paper)\n",
        "        features = tf.keras.layers.Dense(7200, activation='linear', name='features')(dropout1)\n",
        "\n",
        "        # Classification output (for training)\n",
        "        classifier_dense = tf.keras.layers.Dense(256, activation='relu')(dropout1)\n",
        "        classifier = tf.keras.layers.Dense(5, activation='softmax', name='classifier')(classifier_dense)\n",
        "\n",
        "        # Build model\n",
        "        self.model = tf.keras.Model(inputs=inputs, outputs=[features, classifier])\n",
        "\n",
        "        # Compile for training\n",
        "        self.model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "            loss={'features': 'mse', 'classifier': 'categorical_crossentropy'},\n",
        "            loss_weights={'features': 0.3, 'classifier': 0.7},\n",
        "            metrics={'classifier': 'accuracy'}\n",
        "        )\n",
        "\n",
        "        print(\"BiLSTM network built successfully\")\n",
        "        return self.model\n",
        "\n",
        "    def extract_features(self, beats):\n",
        "        \"\"\"\n",
        "        Extract 7200 features from each heartbeat\n",
        "\n",
        "        Input: Array of beats (N x 180)\n",
        "        Output: Array of features (N x 7200)\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built. Call build_network() first.\")\n",
        "\n",
        "        # Normalize beats (important for neural networks)\n",
        "        beats_normalized = []\n",
        "        for beat in beats:\n",
        "            # Fit scaler to each beat individually for real-time processing\n",
        "            beat_scaled = (beat - np.mean(beat)) / (np.std(beat) + 1e-8)\n",
        "            beats_normalized.append(beat_scaled)\n",
        "\n",
        "        # Reshape for LSTM input (N, 180, 1)\n",
        "        beats_array = np.array(beats_normalized).reshape(-1, 180, 1)\n",
        "\n",
        "        # Extract features (ignore classification output for feature extraction)\n",
        "        features, _ = self.model.predict(beats_array, batch_size=32, verbose=0)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def train_on_data(self, X_train, y_train, X_val, y_val, epochs=50):\n",
        "        \"\"\"\n",
        "        Train the BiLSTM on labeled ECG data (like MIT-BIH)\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            self.build_network()\n",
        "\n",
        "        # Prepare training data\n",
        "        X_train_scaled = np.array([\n",
        "            (beat - np.mean(beat)) / (np.std(beat) + 1e-8)\n",
        "            for beat in X_train\n",
        "        ]).reshape(-1, 180, 1)\n",
        "\n",
        "        X_val_scaled = np.array([\n",
        "            (beat - np.mean(beat)) / (np.std(beat) + 1e-8)\n",
        "            for beat in X_val\n",
        "        ]).reshape(-1, 180, 1)\n",
        "\n",
        "        # Create dummy feature targets (model will learn these)\n",
        "        features_train = np.random.normal(0, 1, (len(X_train), 7200))\n",
        "        features_val = np.random.normal(0, 1, (len(X_val), 7200))\n",
        "\n",
        "        # Convert labels to categorical\n",
        "        y_train_cat = tf.keras.utils.to_categorical(y_train, 5)\n",
        "        y_val_cat = tf.keras.utils.to_categorical(y_val, 5)\n",
        "\n",
        "        # Training callbacks\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = self.model.fit(\n",
        "            X_train_scaled,\n",
        "            {'features': features_train, 'classifier': y_train_cat},\n",
        "            validation_data=(X_val_scaled, {'features': features_val, 'classifier': y_val_cat}),\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(f\"Training completed. Best validation accuracy: {max(history.history['val_classifier_accuracy']):.4f}\")\n",
        "        return history"
      ],
      "metadata": {
        "id": "a2lUf56DB2r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureCompressor:\n",
        "    def __init__(self, n_components=150):\n",
        "        self.n_components = n_components\n",
        "        self.pca = PCA(n_components=n_components)\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit_and_transform(self, training_features):\n",
        "        \"\"\"\n",
        "        Learn compression from training data and apply it\n",
        "\n",
        "        Input: Training features (N x 7200)\n",
        "        Output: Compressed features (N x 150)\n",
        "        \"\"\"\n",
        "        # Fit PCA on training data\n",
        "        compressed_features = self.pca.fit_transform(training_features)\n",
        "        self.fitted = True\n",
        "\n",
        "        # Report how much information is retained\n",
        "        variance_retained = np.sum(self.pca.explained_variance_ratio_)\n",
        "        print(f\"PCA compression: 7200 â†’ 150 features\")\n",
        "        print(f\"Information retained: {variance_retained:.1%}\")\n",
        "\n",
        "        return compressed_features\n",
        "\n",
        "    def transform(self, features):\n",
        "        \"\"\"\n",
        "        Apply learned compression to new data\n",
        "        \"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"PCA not fitted. Call fit_and_transform() first.\")\n",
        "\n",
        "        return self.pca.transform(features)\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save the trained PCA model\"\"\"\n",
        "        joblib.dump(self.pca, filepath)\n",
        "        print(f\"PCA model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load a trained PCA model\"\"\"\n",
        "        self.pca = joblib.load(filepath)\n",
        "        self.fitted = True\n",
        "        print(f\"PCA model loaded from {filepath}\")"
      ],
      "metadata": {
        "id": "ehrbk7ERB76L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BeatClassifier:\n",
        "    def __init__(self):\n",
        "        # Random Forest: 100 decision trees voting\n",
        "        self.classifier = RandomForestClassifier(\n",
        "            n_estimators=100,      # 100 decision trees\n",
        "            max_depth=10,          # Prevent overfitting\n",
        "            min_samples_split=5,   # Need 5+ samples to split\n",
        "            min_samples_leaf=2,    # Need 2+ samples in leaf\n",
        "            random_state=42,       # Reproducible results\n",
        "            n_jobs=-1             # Use all CPU cores\n",
        "        )\n",
        "        self.fitted = False\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Train Random Forest on compressed features\n",
        "\n",
        "        Input: Compressed features (N x 150), Labels (N,)\n",
        "        \"\"\"\n",
        "        print(\"Training Random Forest classifier...\")\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.fitted = True\n",
        "\n",
        "        # Report training accuracy\n",
        "        train_accuracy = self.classifier.score(X_train, y_train)\n",
        "        print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    def predict_probabilities(self, features):\n",
        "        \"\"\"\n",
        "        Get probability distribution over all classes\n",
        "\n",
        "        Output: Probabilities for [Normal, SVE, VEB, Fusion, Unknown]\n",
        "        \"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Classifier not trained.\")\n",
        "\n",
        "        return self.classifier.predict_proba(features)\n",
        "\n",
        "    def predict_classes(self, features):\n",
        "        \"\"\"Get most likely class for each beat\"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Classifier not trained.\")\n",
        "\n",
        "        return self.classifier.predict(features)\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save trained classifier\"\"\"\n",
        "        joblib.dump(self.classifier, filepath)\n",
        "        print(f\"Classifier saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load trained classifier\"\"\"\n",
        "        self.classifier = joblib.load(filepath)\n",
        "        self.fitted = True\n",
        "        print(f\"Classifier loaded from {filepath}\")"
      ],
      "metadata": {
        "id": "xlz9mGRWCuli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CardiacRiskEngine:\n",
        "    def __init__(self, window_minutes=10, prediction_hours=6):\n",
        "        self.window_minutes = window_minutes\n",
        "        self.prediction_hours = prediction_hours\n",
        "\n",
        "        # Store recent beat predictions\n",
        "        self.beat_history = deque(maxlen=window_minutes * 60)  # 1 beat per second max\n",
        "\n",
        "        # Risk prediction model (trained on outcomes)\n",
        "        self.risk_predictor = LogisticRegression()\n",
        "        self.risk_model_trained = False\n",
        "\n",
        "    def add_beat_prediction(self, timestamp, class_probabilities):\n",
        "        \"\"\"\n",
        "        Add a new beat classification to history\n",
        "\n",
        "        Input:\n",
        "        - timestamp: when this beat occurred\n",
        "        - class_probabilities: [P(Normal), P(SVE), P(VEB), P(Fusion), P(Unknown)]\n",
        "        \"\"\"\n",
        "        # Calculate risk score for this individual beat\n",
        "        beat_risk = sum(prob * RISK_WEIGHTS[CLASSES[i]]\n",
        "                       for i, prob in enumerate(class_probabilities))\n",
        "\n",
        "        # Store beat information\n",
        "        beat_data = {\n",
        "            'timestamp': timestamp,\n",
        "            'probabilities': class_probabilities,\n",
        "            'risk_score': beat_risk,\n",
        "            'dominant_class': CLASSES[np.argmax(class_probabilities)]\n",
        "        }\n",
        "\n",
        "        self.beat_history.append(beat_data)\n",
        "\n",
        "    def calculate_window_risk_features(self):\n",
        "        \"\"\"\n",
        "        Calculate risk features from recent beat history\n",
        "\n",
        "        This is WHERE THE MAGIC HAPPENS - turning beat classifications into risk features\n",
        "        \"\"\"\n",
        "        if len(self.beat_history) < 10:\n",
        "            return None  # Need minimum beats for analysis\n",
        "\n",
        "        recent_beats = list(self.beat_history)\n",
        "\n",
        "        # Count frequency of each arrhythmia type\n",
        "        class_counts = {class_name: 0 for class_name in CLASSES.values()}\n",
        "        risk_scores = []\n",
        "\n",
        "        for beat in recent_beats:\n",
        "            # Add to class counts based on most likely class\n",
        "            class_counts[beat['dominant_class']] += 1\n",
        "            risk_scores.append(beat['risk_score'])\n",
        "\n",
        "        total_beats = len(recent_beats)\n",
        "\n",
        "        # Calculate comprehensive risk features\n",
        "        features = {\n",
        "            # Frequency features (percentage of each arrhythmia type)\n",
        "            'normal_frequency': class_counts['Normal'] / total_beats,\n",
        "            'sve_frequency': class_counts['SVE'] / total_beats,\n",
        "            'veb_frequency': class_counts['VEB'] / total_beats,\n",
        "            'fusion_frequency': class_counts['Fusion'] / total_beats,\n",
        "            'unknown_frequency': class_counts['Unknown'] / total_beats,\n",
        "\n",
        "            # Risk burden features\n",
        "            'mean_risk_score': np.mean(risk_scores),\n",
        "            'max_risk_score': np.max(risk_scores),\n",
        "            'std_risk_score': np.std(risk_scores),\n",
        "\n",
        "            # Dangerous arrhythmia burden\n",
        "            'high_risk_burden': sum(1 for score in risk_scores if score > 5.0) / total_beats,\n",
        "            'critical_arrhythmia_rate': (class_counts['VEB'] + class_counts['Fusion']) / total_beats,\n",
        "\n",
        "            # Trend analysis (is situation getting worse?)\n",
        "            'risk_trend': self._calculate_trend(risk_scores),\n",
        "\n",
        "            # Pattern complexity\n",
        "            'arrhythmia_variety': sum(1 for count in class_counts.values() if count > 0) - 1,  # Subtract normal\n",
        "\n",
        "            # Total analysis window\n",
        "            'total_beats_analyzed': total_beats\n",
        "        }\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _calculate_trend(self, risk_scores):\n",
        "        \"\"\"\n",
        "        Calculate if risk is increasing or decreasing over time\n",
        "        Positive = getting worse, Negative = getting better\n",
        "        \"\"\"\n",
        "        if len(risk_scores) < 5:\n",
        "            return 0.0\n",
        "\n",
        "        # Simple linear regression slope\n",
        "        x = np.arange(len(risk_scores))\n",
        "        slope, _ = np.polyfit(x, risk_scores, 1)\n",
        "        return slope\n",
        "\n",
        "    def predict_cardiac_event_risk(self):\n",
        "        \"\"\"\n",
        "        Predict probability of cardiac event in next 6 hours\n",
        "\n",
        "        Returns: Probability between 0.0 and 1.0\n",
        "        \"\"\"\n",
        "        features = self.calculate_window_risk_features()\n",
        "        if features is None:\n",
        "            return 0.0\n",
        "\n",
        "        if self.risk_model_trained:\n",
        "            # Use trained ML model\n",
        "            feature_vector = np.array([list(features.values())]).reshape(1, -1)\n",
        "            event_probability = self.risk_predictor.predict_proba(feature_vector)[0][1]\n",
        "            return event_probability\n",
        "        else:\n",
        "            # Use clinical rules if no trained model\n",
        "            return self._rule_based_risk_assessment(features)\n",
        "\n",
        "    def _rule_based_risk_assessment(self, features):\n",
        "        \"\"\"\n",
        "        Clinical rule-based risk assessment\n",
        "        Based on established cardiology guidelines\n",
        "        \"\"\"\n",
        "        risk_score = 0.0\n",
        "\n",
        "        # Rule 1: High VEB frequency (>10% is concerning)\n",
        "        if features['veb_frequency'] > 0.10:\n",
        "            risk_score += 0.4\n",
        "        elif features['veb_frequency'] > 0.05:\n",
        "            risk_score += 0.2\n",
        "\n",
        "        # Rule 2: Any fusion beats (very concerning)\n",
        "        if features['fusion_frequency'] > 0.02:  # >2%\n",
        "            risk_score += 0.5\n",
        "\n",
        "        # Rule 3: High overall arrhythmia burden\n",
        "        if features['critical_arrhythmia_rate'] > 0.15:  # >15%\n",
        "            risk_score += 0.3\n",
        "\n",
        "        # Rule 4: Increasing risk trend (getting worse)\n",
        "        if features['risk_trend'] > 0.1:\n",
        "            risk_score += 0.3\n",
        "\n",
        "        # Rule 5: Very high individual risk episodes\n",
        "        if features['max_risk_score'] > 8.0:\n",
        "            risk_score += 0.2\n",
        "\n",
        "        # Rule 6: High risk burden over time\n",
        "        if features['high_risk_burden'] > 0.20:  # >20% high-risk beats\n",
        "            risk_score += 0.3\n",
        "\n",
        "        # Rule 7: Multiple arrhythmia types (electrical instability)\n",
        "        if features['arrhythmia_variety'] >= 3:\n",
        "            risk_score += 0.2\n",
        "\n",
        "        # Cap at 100%\n",
        "        return min(risk_score, 1.0)\n",
        "\n",
        "    def train_risk_model(self, historical_data):\n",
        "        \"\"\"\n",
        "        Train ML risk model on historical patient data\n",
        "\n",
        "        historical_data: List of cases with features and known outcomes\n",
        "        Format: [{'features': {...}, 'had_event': True/False}, ...]\n",
        "        \"\"\"\n",
        "        if len(historical_data) < 50:\n",
        "            print(\"Need at least 50 cases for risk model training\")\n",
        "            return\n",
        "\n",
        "        # Prepare training data\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for case in historical_data:\n",
        "            X.append(list(case['features'].values()))\n",
        "            y.append(1 if case['had_event'] else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Train logistic regression\n",
        "        self.risk_predictor.fit(X, y)\n",
        "        self.risk_model_trained = True\n",
        "\n",
        "        # Report performance\n",
        "        accuracy = self.risk_predictor.score(X, y)\n",
        "        print(f\"Risk model trained on {len(historical_data)} cases\")\n",
        "        print(f\"Training accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "IhHEe2sHCvat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_complete_system():\n",
        "    \"\"\"\n",
        "    Complete training pipeline for the cardiac prediction system\n",
        "    \"\"\"\n",
        "    print(\"ðŸŽ¯ Starting complete system training...\")\n",
        "\n",
        "    # Step 1: Load MIT-BIH training data\n",
        "    # (You'll need to implement this based on your data format)\n",
        "    print(\"ðŸ“Š Loading MIT-BIH Arrhythmia Database...\")\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = load_mitbih_data()\n",
        "\n",
        "    # Step 2: Train BiLSTM feature extractor\n",
        "    print(\"ðŸ§  Training BiLSTM feature extractor...\")\n",
        "    feature_extractor = BiLSTMFeatureExtractor()\n",
        "    feature_extractor.build_network()\n",
        "\n",
        "    # Train on beat classification task\n",
        "    history = feature_extractor.train_on_data(X_train, y_train, X_val, y_val, epochs=50)\n",
        "\n",
        "    # Save BiLSTM weights\n",
        "    feature_extractor.model.save_weights('models/bilstm_weights.h5')\n",
        "    print(\"âœ… BiLSTM training complete\")\n",
        "\n",
        "    # Step 3: Extract features for all data\n",
        "    print(\"ðŸ” Extracting features from all data...\")\n",
        "    train_features = feature_extractor.extract_features(X_train)\n",
        "    val_features = feature_extractor.extract_features(X_val)\n",
        "    test_features = feature_extractor.extract_features(X_test)\n",
        "\n",
        "    # Step 4: Train PCA compressor\n",
        "    print(\"ðŸ“ Training PCA feature compressor...\")\n",
        "    compressor = FeatureCompressor(n_components=150)\n",
        "    train_features_compressed = compressor.fit_and_transform(train_features)\n",
        "    val_features_compressed = compressor.transform(val_features)\n",
        "    test_features_compressed = compressor.transform(test_features)\n",
        "\n",
        "    # Save PCA model\n",
        "    compressor.save_model('models/pca_model.pkl')\n",
        "\n",
        "    # Step 5: Train Random Forest classifier\n",
        "    print(\"ðŸŒ² Training Random Forest classifier...\")\n",
        "    classifier = BeatClassifier()\n",
        "    classifier.train(train_features_compressed, y_train)\n",
        "\n",
        "    # Evaluate performance\n",
        "    val_predictions = classifier.predict_classes(val_features_compressed)\n",
        "    accuracy = np.mean(val_predictions == y_val)\n",
        "    print(f\"âœ… Validation accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Save classifier\n",
        "    classifier.save_model('models/rf_classifier.pkl')\n",
        "\n",
        "    # Step 6: Create synthetic risk training data\n",
        "    # (In real deployment, you'd use historical patient outcomes)\n",
        "    print(\"âš¡ Training cardiac risk prediction model...\")\n",
        "    risk_engine = CardiacRiskEngine()\n",
        "\n",
        "    # Generate synthetic outcomes based on arrhythmia patterns\n",
        "    synthetic_risk_data = create_synthetic_risk_data(test_features_compressed, y_test)\n",
        "    risk_engine.train_risk_model(synthetic_risk_data)\n",
        "\n",
        "    print(\"ðŸŽ‰ Complete system training finished!\")\n",
        "\n",
        "    return {\n",
        "        'bilstm_weights': 'models/bilstm_weights.h5',\n",
        "        'pca_model': 'models/pca_model.pkl',\n",
        "        'classifier': 'models/rf_classifier.pkl'\n",
        "    }\n",
        "\n",
        "def load_mitbih_data():\n",
        "    \"\"\"\n",
        "    Load MIT-BIH Arrhythmia Database\n",
        "    (Implement this based on your data source)\n",
        "    \"\"\"\n",
        "    # This is a placeholder - implement based on your data format\n",
        "    # You can download MIT-BIH from PhysioNet or use the Kaggle dataset\n",
        "    # from the paper: https://www.kaggle.com/taejoongyoon/mitbit-arrhythmia-database\n",
        "\n",
        "    path = kagglehub.dataset_download(\"taejoongyoon/mitbit-arrhythmia-database\")\n",
        "    return path\n",
        "\n",
        "def create_synthetic_risk_data(features, labels):\n",
        "    \"\"\"\n",
        "    Create synthetic risk training data\n",
        "    (In production, use real patient outcomes)\n",
        "    \"\"\"\n",
        "    risk_cases = []\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        # Simulate risk features based on beat classifications\n",
        "        window_features = {\n",
        "            'normal_frequency': 0.8 if labels[i] == 0 else 0.4,\n",
        "            'sve_frequency': 0.05 if labels[i] == 1 else 0.02,\n",
        "            'veb_frequency': 0.15 if labels[i] == 2 else 0.03,\n",
        "            'fusion_frequency': 0.10 if labels[i] == 3 else 0.01,\n",
        "            'unknown_frequency': 0.05,\n",
        "            'mean_risk_score': 4.0 if labels[i] > 0 else 1.0,\n",
        "            'max_risk_score': 8.0 if labels[i] in [2, 3] else 3.0,\n",
        "            'std_risk_score': 2.0,\n",
        "            'high_risk_burden': 0.3 if labels[i] > 0 else 0.1,\n",
        "            'critical_arrhythmia_rate': 0.25 if labels[i] in [2, 3] else 0.05,\n",
        "            'risk_trend': 0.1 if labels[i] > 0 else -0.05,\n",
        "            'arrhythmia_variety': 2 if labels[i] > 0 else 1,\n",
        "            'total_beats_analyzed': 100\n",
        "        }\n",
        "\n",
        "        # Simulate cardiac event occurrence (higher for dangerous arrhythmias)\n",
        "        event_probability = 0.15 if labels[i] in [2, 3] else 0.03\n",
        "        had_event = np.random.random() < event_probability\n",
        "\n",
        "        risk_cases.append({\n",
        "            'features': window_features,\n",
        "            'had_event': had_event\n",
        "        })\n",
        "\n",
        "    return risk_cases"
      ],
      "metadata": {
        "id": "H_o5gKpiEiGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYnbxO7AEkCe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}