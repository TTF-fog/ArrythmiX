{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlPvSqLjSRw_",
        "outputId": "d8c9fe8d-b1fd-4712-b17e-af9bc452b787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.19 in /usr/local/lib/python3.12/dist-packages (from tensorflow[and-cuda]==2.19) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19->tensorflow[and-cuda]==2.19) (0.5.3)\n",
            "Collecting nvidia-cublas-cu12==12.5.3.2 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_cublas_cu12-12.5.3.2-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.5.82 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.5.82 in /usr/local/lib/python3.12/dist-packages (from tensorflow[and-cuda]==2.19) (12.5.82)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.5.82 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.5.82 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.3.0.75 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_cudnn_cu12-9.3.0.75-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.3.61 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_cufft_cu12-11.2.3.61-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.6.82 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_curand_cu12-10.3.6.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.3.83 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.3.83-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.1.3 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.23.4 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.5.82 (from tensorflow[and-cuda]==2.19)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19->tensorflow[and-cuda]==2.19) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.5.3.2-py3-none-manylinux2014_x86_64.whl (363.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.3/363.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (24.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (895 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.7/895.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.3.0.75-py3-none-manylinux2014_x86_64.whl (577.2 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.1/577.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:02:53\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
            "    data: bytes = self.__fp.read(amt)\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 479, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1251, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1103, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
            "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
            "    self._complete_partial_requirements(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
            "    for link, (filepath, _) in batch_download:\n",
            "                               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/network/download.py\", line 184, in __call__\n",
            "    for chunk in chunks:\n",
            "                 ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!python3 -m pip install tensorflow[and-cuda]==2.19\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy"
      ],
      "metadata": {
        "id": "5p4sKzOK0uFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b05c4f-9940-4d34-ba6e-0eb07e94ca89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wfdb\n",
        "!wget -r -N -c -np https://physionet.org/files/sddb/1.0.0/\n",
        "!which python"
      ],
      "metadata": {
        "id": "o19OQ-bbsqLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ3hXR7tTlVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f6166a-63c5-45f3-de06-819bd4fc3898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.2)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.signal import butter, filtfilt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import wfdb\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7zum4zNTnr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e76b337-3545-445a-cb97-22371c31b696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n",
            "loaded file\n"
          ]
        }
      ],
      "source": [
        "fs = 250  # Sampling frequency\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def bandpass_filter(data, lowcut=0.5, highcut=100, fs=250, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def load_sddb_lead_ii(file_number):\n",
        "    record = wfdb.rdrecord(f'/content/physionet.org/files/sddb/1.0.0/{file_number}')\n",
        "    signal = record.p_signal[:, 1]\n",
        "    filtered = bandpass_filter(signal)\n",
        "    normalized = (filtered - np.mean(filtered)) / np.std(filtered)\n",
        "    return normalized\n",
        "ecg_signals = []\n",
        "for file_num in range(30, 50):\n",
        "    try:\n",
        "        ecg = load_sddb_lead_ii(file_num)\n",
        "        ecg_signals.append(ecg)\n",
        "        print(\"loaded file\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_num}: {e}\")\n",
        "\n",
        "for file_num in range(52, 53):\n",
        "    try:\n",
        "        ecg = load_sddb_lead_ii(file_num)\n",
        "        ecg_signals.append(ecg)\n",
        "        print(\"loaded file\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_num}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgjxFKrAjhLh"
      },
      "source": [
        "data iwndow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "495649fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b117e2d7-ba06-43e0-e273-07e206043177"
      },
      "source": [
        "vf_onset_times_in_samples = {\n",
        "    30: 28473,   # 07:54:33\n",
        "    31: 49344,   # 13:42:24\n",
        "    32: 60318,   # 16:45:18\n",
        "    33: 17179,   # 04:46:19\n",
        "    34: 23744,   # 06:35:44\n",
        "    35: 88496,   # 24:34:56\n",
        "    36: 68341,   # 18:59:01\n",
        "    37: 5473,    # 01:31:13\n",
        "    38: 28914,   # 08:01:54   (VF onset present)\n",
        "    39: 16671,   # 04:37:51\n",
        "    40: None,    # paced, no VF\n",
        "    41: 10764,   # 02:59:24\n",
        "    42: None,    # no VF\n",
        "    43: 56231,   # 15:37:11\n",
        "    44: 70725,   # 19:38:45\n",
        "    45: 65357,   # 18:09:17\n",
        "    46: 13307,   # 03:41:47\n",
        "    47: 22381,   # 06:13:01\n",
        "    48: 8980,    # 02:29:40\n",
        "    49: None,    # paced, no VF\n",
        "    50: 42343,   # 11:45:43\n",
        "    51: 82703,   # 22:58:23\n",
        "    52: 9160     # 02:32:40\n",
        "}\n",
        "\n",
        "print(\"VF onset times in samples dictionary created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VF onset times in samples dictionary created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d84e860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "8c95a74f-0bdb-44db-adba-031e49a00b34"
      },
      "source": [
        "window_size = fs * 60  # 5 minutes\n",
        "stride = fs * 150  # 50% overlap\n",
        "\n",
        "def get_vf_onset_time(file_number):\n",
        "  vf_onset_sample = vf_onset_times_in_samples.get(file_number)\n",
        "  if vf_onset_sample is not None:\n",
        "      print(f\"Retrieved VF onset sample for file {file_number}: {vf_onset_sample}\")\n",
        "  else:\n",
        "      print(f\"No VF onset sample found for file {file_number}, or it is None.\")\n",
        "  return vf_onset_sample\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "for file_num, signal in zip(range(30, 53), ecg_signals):\n",
        "    vf_sample = get_vf_onset_time(file_num)\n",
        "\n",
        "    if len(signal) < window_size:\n",
        "        print(f\"Skipping file {file_num} due to insufficient signal length.\")\n",
        "        continue\n",
        "\n",
        "    for start in range(0, len(signal) - window_size + 1, stride):\n",
        "        window = signal[start:start + window_size]\n",
        "        print(f\"start time {start}\")\n",
        "        # Ensure window has correct size, if not, skip\n",
        "        if len(window) != window_size:\n",
        "            print(f\"Skipping window due to incorrect size: {len(window)} instead of {window_size}\")\n",
        "            continue\n",
        "\n",
        "        window_center = start + window_size // 2\n",
        "        print(f\"window center {window_center}\")\n",
        "        if vf_sample is not None:\n",
        "          time_diff_sec = vf_sample\n",
        "        else:\n",
        "          time_diff_sec = None\n",
        "        if time_diff_sec == None:\n",
        "          risk_class = 0\n",
        "        elif time_diff_sec < 0:\n",
        "            continue  # after VF event, ignore\n",
        "        elif time_diff_sec > 24 * 3600:\n",
        "            risk_class = 0  # low risk (>24 hrs before VF)\n",
        "        elif time_diff_sec > 12 * 3600:\n",
        "            risk_class = 1  # medium risk (12-24 hrs)\n",
        "        elif time_diff_sec > 6 * 3600:\n",
        "            risk_class = 2  # high risk (6-12 hrs)\n",
        "        else:\n",
        "            risk_class = 3  # critical risk (<6 hrs)\n",
        "\n",
        "        X.append(window)\n",
        "        y.append(risk_class)\n",
        "print(y)\n",
        "X = np.array(X).reshape(-1, window_size, 1)  # Add channel dimension\n",
        "y = np.array(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3206411023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrisk_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add channel dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0241126"
      },
      "source": [
        "**Reasoning**:\n",
        "The `X` and `y` arrays have been regenerated with the corrected VF onset times and updated risk class assignments. The next step is to re-apply the data augmentation to these new `X` and `y` arrays, as done previously in the notebook, to ensure the training data is appropriately augmented before splitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d25d37d"
      },
      "source": [
        "def augment_ecg(ecg):\n",
        "    shift = np.random.randint(-fs*5, fs*5)\n",
        "    ecg = np.roll(ecg, shift, axis=0)\n",
        "\n",
        "    scale = np.random.uniform(0.95, 1.05)\n",
        "    ecg = ecg * scale\n",
        "\n",
        "    noise = np.random.normal(0, 0.01, size=ecg.shape)\n",
        "    ecg = ecg + noise\n",
        "\n",
        "    return ecg\n",
        "\n",
        "X_aug, y_aug = [], []\n",
        "\n",
        "for i in range(len(X)):\n",
        "    for _ in range(2):\n",
        "        X_aug.append(augment_ecg(np.copy(X[i])))\n",
        "        y_aug.append(y[i])\n",
        "\n",
        "X = np.concatenate([X, np.array(X_aug)], axis=0)\n",
        "y = np.concatenate([y, np.array(y_aug)], axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "858c019f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.05, stratify=y, random_state=42)\n",
        "val_ratio = 0.15 / 0.95\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_ratio, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(f\"Train size: {X_train.shape[0]}, Validation size: {X_val.shape[0]}, Test size: {X_test.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8190c4d1"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6078d0e"
      },
      "source": [
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "\n",
        "valid_mask = (~np.any(np.isnan(X_train_flat), axis=1)) & (np.any(X_train_flat != 0, axis=1))\n",
        "\n",
        "X_train_valid = X_train_flat[valid_mask]\n",
        "y_train_valid = y_train[valid_mask]\n",
        "\n",
        "X_train_invalid = X_train_flat[~valid_mask]\n",
        "y_train_invalid = y_train[~valid_mask]\n",
        "\n",
        "# Check class distribution in valid data\n",
        "import numpy as np\n",
        "unique, counts = np.unique(y_train_valid, return_counts=True)\n",
        "print(f'Classes in SMOTE-valid data: {dict(zip(unique, counts))}')\n",
        "\n",
        "# Apply SMOTE on only valid subset if >1 class present\n",
        "from imblearn.over_sampling import SMOTE\n",
        "if len(unique) > 1:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_res_valid, y_res_valid = smote.fit_resample(X_train_valid, y_train_valid)\n",
        "else:\n",
        "    print(\"Warning: Only one class in SMOTE-valid data. Skipping SMOTE.\")\n",
        "    X_res_valid, y_res_valid = X_train_valid, y_train_valid\n",
        "\n",
        "X_res_valid = X_res_valid.reshape(-1, window_size, 1)\n",
        "\n",
        "X_train_clean = np.concatenate([X_res_valid, X_train_invalid.reshape(-1, window_size, 1)], axis=0)\n",
        "y_train_clean = np.concatenate([y_res_valid, y_train_invalid], axis=0)\n",
        "\n",
        "print(f'Final training set after SMOTE and reintegrating invalid windows: {X_train_clean.shape[0]} samples')\n",
        "\n",
        "import pandas as pd\n",
        "print(\"\\nClass distribution in y_train_clean:\")\n",
        "print(pd.Series(y_train_clean).value_counts())\n",
        "\n",
        "print(\"\\nClass distribution in y_val:\")\n",
        "print(pd.Series(y_val).value_counts())\n",
        "\n",
        "print(\"\\nClass distribution in y_test:\")\n",
        "print(pd.Series(y_test).value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_diffs = []\n",
        "\n",
        "for file_num, signal in zip(range(30, 53), ecg_signals):\n",
        "    vf_sample = vf_onset_times_in_samples.get(file_num)\n",
        "\n",
        "    # Only consider signals that are long enough for at least one window\n",
        "    if len(signal) < window_size:\n",
        "        continue\n",
        "\n",
        "    for start in range(0, len(signal) - window_size + 1, stride):\n",
        "        window_center = start + window_size // 2\n",
        "        if vf_sample == None:\n",
        "          vf_sample = 0\n",
        "        time_diffs.append(vf_sample)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(time_diffs, bins=50, edgecolor='black')\n",
        "plt.title('Distribution of Time Differences (seconds) from VF Onset (with updated window size)')\n",
        "plt.xlabel('Time Difference to VF Onset (seconds)')\n",
        "plt.ylabel('Number of Windows')\n",
        "\n",
        "# Original risk thresholds for reference\n",
        "plt.axvline(x=6 * 3600, color='r', linestyle=':', label='Original 6 hours (21600s) threshold')\n",
        "plt.axvline(x=12 * 3600, color='g', linestyle=':', label='Original 12 hours (43200s) threshold')\n",
        "plt.axvline(x=24 * 3600, color='b', linestyle=':', label='Original 24 hours (86400s) threshold')\n",
        "\n",
        "# Current risk thresholds for reference (from cell 5d84e860)\n",
        "plt.axvline(x=6 * 3600, color='r', linestyle='--', label='Current 6 hours (21600s) threshold')\n",
        "plt.axvline(x=12 * 3600, color='g', linestyle='--', label='Current 12 hours (43200s) threshold')\n",
        "plt.axvline(x=24 * 3600, color='b', linestyle='--', label='Current 24 hours (86400s) threshold')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Minimum time difference: {np.min(time_diffs)} seconds\")\n",
        "print(f\"Maximum time difference: {np.max(time_diffs)} seconds\")\n",
        "print(f\"Mean time difference: {np.mean(time_diffs)} seconds\")\n",
        "print(f\"Median time difference: {np.median(time_diffs)} seconds\")"
      ],
      "metadata": {
        "id": "xzjtsM9lE6Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = X_train_clean.shape[1]\n",
        "inputs = Input(shape=(window_size, 1))\n",
        "\n",
        "x = Conv1D(64, 5, activation='relu', padding='same')(inputs)\n",
        "x = MaxPooling1D(pool_size=4)(x)\n",
        "x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "x = MaxPooling1D(pool_size=4)(x)\n",
        "\n",
        "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
        "\n",
        "attn_output = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
        "x = LayerNormalization()(attn_output)\n",
        "\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "y_train_cat = to_categorical(y_train_clean, 4)\n",
        "y_val_cat = to_categorical(y_val, 4)\n",
        "y_test_cat = to_categorical(y_test, 4)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=1000, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(factor=0.5, patience=5),\n",
        "    ModelCheckpoint('sddb_single_lead_best_model.h5', save_best_only=True, monitor='val_loss')\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_clean, y_train_cat,\n",
        "    validation_data=(X_val, y_val_cat),\n",
        "    epochs=500,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "v-P0u05_CxoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb608867"
      },
      "source": [
        "# Task\n",
        "The model shows 'nan' loss, which indicates a problem with the data. I'll modify the `load_sddb_lead_ii` function to handle cases where the signal's standard deviation is zero by returning `None` for such invalid signals. I'll also update the loops that populate `ecg_signals` to store valid signals in a dictionary, preventing NaNs from entering the dataset. Finally, I will re-run the subsequent data processing and model training steps.\n",
        "\n",
        "I'll start by modifying the `load_sddb_lead_ii` function in cell `w7zum4zNTnr8` to properly handle potential NaN values by checking the standard deviation before normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "102cac63"
      },
      "source": [
        "## Refine Data Loading and NaN Handling\n",
        "\n",
        "### Subtask:\n",
        "Modify the `load_sddb_lead_ii` function in cell `w7zum4zNTnr8` to handle cases where the signal's standard deviation is zero after filtering. This function will now return `None` for such invalid signals. Update the loops that populate `ecg_signals` to store valid signals in a dictionary, `ecg_signals_dict`, mapping `file_num` to the signal, ensuring proper alignment and skipping of invalid signals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "783578b9"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `load_sddb_lead_ii` function to handle zero standard deviation and changing how ECG signals are stored from a list to a dictionary, skipping invalid signals. This change must be applied to the existing code cell `w7zum4zNTnr8`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uFPQmLydQp6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a92b596"
      },
      "source": [
        "fs = 250  # Sampling frequency\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def bandpass_filter(data, lowcut=0.5, highcut=100, fs=250, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def load_sddb_lead_ii(file_number):\n",
        "    record = wfdb.rdrecord(f'/content/dataset/{file_number}')\n",
        "    signal = record.p_signal[:, 1]  # Lead II is channel index 1\n",
        "    filtered = bandpass_filter(signal)\n",
        "\n",
        "    # Add check for zero standard deviation\n",
        "    if np.std(filtered) == 0:\n",
        "        print(f\"Warning: Standard deviation is zero for file {file_number}. Returning None.\")\n",
        "        return None\n",
        "\n",
        "    normalized = (filtered - np.mean(filtered)) / np.std(filtered)\n",
        "    return normalized\n",
        "\n",
        "ecq_signals_dict = {}\n",
        "for file_num in range(30, 50):\n",
        "    try:\n",
        "        ecg = load_sddb_lead_ii(file_num)\n",
        "        if ecg is not None:\n",
        "            ecq_signals_dict[file_num] = ecg\n",
        "            print(f\"Loaded file {file_num}\")\n",
        "        else:\n",
        "            print(f\"Skipping file {file_num} due to invalid signal.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_num}: {e}\")\n",
        "\n",
        "for file_num in range(52, 53):\n",
        "    try:\n",
        "        ecg = load_sddb_lead_ii(file_num)\n",
        "        if ecg is not None:\n",
        "            ecq_signals_dict[file_num] = ecg\n",
        "            print(f\"Loaded file {file_num}\")\n",
        "        else:\n",
        "            print(f\"Skipping file {file_num} due to invalid signal.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_num}: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5a065d8"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `load_sddb_lead_ii` function to handle zero standard deviation and changing how ECG signals are stored from a list to a dictionary, skipping invalid signals. This change must be applied to the existing code cell `w7zum4zNTnr8`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f1f39b7"
      },
      "source": [
        "fs = 250  # Sampling frequency\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def bandpass_filter(data, lowcut=0.5, highcut=100, fs=250, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def load_sddb_lead_ii(file_number):\n",
        "    record = wfdb.rdrecord(f'/content/dataset/{file_number}')\n",
        "    signal = record.p_signal[:, 1]  # Lead II is channel index 1\n",
        "    filtered = bandpass_filter(signal)\n",
        "\n",
        "    # Add check for zero standard deviation\n",
        "    if np.std(filtered) == 0:\n",
        "        print(f\"Warning: Standard deviation is zero for file {file_number}. Returning None.\")\n",
        "        return None\n",
        "\n",
        "    normalized = (filtered - np.mean(filtered)) / np.std(filtered)\n",
        "    return normalized\n",
        "\n",
        "ecq_signals_dict = {}\n",
        "for file_num in range(30, 50):\n",
        "    try:\n",
        "        ecg = load_sddb_lead_ii(file_num)\n",
        "        if ecg is not None:\n",
        "            ecq_signals_dict[file_num] = ecg\n",
        "            print(f\"Loaded file {file_num}\")\n",
        "        else:\n",
        "            print(f\"Skipping file {file_num} due to invalid signal.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_num}: {e}\")\n",
        "\n",
        "for file_num in range(52, 53):\n",
        "    try:\n",
        "        ecg = load_sddb_lead_ii(file_num)\n",
        "        if ecg is not None:\n",
        "            ecq_signals_dict[file_num] = ecg\n",
        "            print(f\"Loaded file {file_num}\")\n",
        "        else:\n",
        "            print(f\"Skipping file {file_num} due to invalid signal.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_num}: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75816a1"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `load_sddb_lead_ii` function to handle zero standard deviation and changing how ECG signals are stored from a list to a dictionary, skipping invalid signals. This change must be applied to the existing code cell `w7zum4zNTnr8`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DDIOSDAHUIOASJKD ASD\\\\ IDHAR SE SHURU KAR**"
      ],
      "metadata": {
        "id": "XtQ3aYVAnvF_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce12414d"
      },
      "source": [
        "fs = 250  # Sampling frequency\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def bandpass_filter(data, lowcut=0.5, highcut=100, fs=250, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def load_sddb_lead_ii(file_number):\n",
        "    record = wfdb.rdrecord(f'/content/dataset/{file_number}')\n",
        "    signal = record.p_signal[:, 1]  # Lead II is channel index 1\n",
        "    filtered = bandpass_filter(signal)\n",
        "\n",
        "    # Add check for zero standard deviation\n",
        "    if np.std(filtered) == 0:\n",
        "        print(f\"Warning: Standard deviation is zero for file {file_number}. Returning None.\")\n",
        "        return None\n",
        "\n",
        "    normalized = (filtered - np.mean(filtered)) / np.std(filtered)\n",
        "    return normalized\n",
        "\n",
        "ecq_signals_dict = {}\n",
        "for file_num in range(30, 50):\n",
        "    try:\n",
        "        ecg = load_sddb_lead_ii(file_num)\n",
        "        if ecg is not None:\n",
        "            ecq_signals_dict[file_num] = ecg\n",
        "            print(f\"Loaded file {file_num}\")\n",
        "        else:\n",
        "            print(f\"Skipping file {file_num} due to invalid signal.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_num}: {e}\")\n",
        "\n",
        "for file_num in range(52, 53):\n",
        "    try:\n",
        "        ecg = load_sddb_lead_ii(file_num)\n",
        "        if ecg is not None:\n",
        "            ecq_signals_dict[file_num] = ecg\n",
        "            print(f\"Loaded file {file_num}\")\n",
        "        else:\n",
        "            print(f\"Skipping file {file_num} due to invalid signal.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_num}: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8d8fbc2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully modified the `load_sddb_lead_ii` function and created `ecq_signals_dict`. Now, I need to update the data windowing and risk class assignment logic to use this new dictionary and handle `None` values correctly, regenerating the `X` and `y` arrays with valid data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cb31137"
      },
      "source": [
        "window_size = fs * 60  # 5 minutes\n",
        "stride = fs * 150  # 50% overlap\n",
        "\n",
        "def get_vf_onset_time(file_number):\n",
        "  # Use the pre-defined dictionary for VF onset times\n",
        "  vf_onset_sample = vf_onset_times_in_samples.get(file_number)\n",
        "  if vf_onset_sample is not None:\n",
        "      print(f\"Retrieved VF onset sample for file {file_number}: {vf_onset_sample}\")\n",
        "  else:\n",
        "      print(f\"No VF onset sample found for file {file_number}, or it is None.\")\n",
        "  return vf_onset_sample\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "for file_num, signal in ecq_signals_dict.items():\n",
        "    vf_sample = get_vf_onset_time(file_num)\n",
        "\n",
        "    # Only consider signals that are long enough for at least one window\n",
        "    if len(signal) < window_size:\n",
        "        print(f\"Skipping file {file_num} due to insufficient signal length.\")\n",
        "        continue\n",
        "\n",
        "    for start in range(0, len(signal) - window_size + 1, stride):\n",
        "        window = signal[start:start + window_size]\n",
        "\n",
        "        # Ensure window has correct size, if not, skip\n",
        "        if len(window) != window_size:\n",
        "            print(f\"Skipping window due to incorrect size: {len(window)} instead of {window_size}\")\n",
        "            continue\n",
        "\n",
        "        window_center = start + window_size // 2\n",
        "\n",
        "        # Calculate time difference from window center to VF onset in samples\n",
        "        # Convert to seconds for comparison with thresholds\n",
        "        if vf_sample is not None:\n",
        "            # If vf_sample exists, calculate time difference from window_center to vf_sample in seconds\n",
        "            time_diff_samples = vf_sample * fs\n",
        "            time_diff_sec = time_diff_samples / fs\n",
        "        else:\n",
        "            # If no VF onset, assign class 0 (low risk)\n",
        "            time_diff_sec = None\n",
        "\n",
        "        # Assign new risk classes based on adjusted time differences (seconds)\n",
        "        if time_diff_sec is None:\n",
        "            risk_class = 0  # Low risk (no VF onset)\n",
        "        elif time_diff_sec < 0: # This case means the window is after the VF event\n",
        "            continue  # Skip windows after the VF event\n",
        "        elif time_diff_sec > 24 * 3600:\n",
        "            risk_class = 0  # low risk (>24 hrs before VF)\n",
        "        elif time_diff_sec > 12 * 3600:\n",
        "            risk_class = 1  # medium risk (12-24 hrs)\n",
        "        elif time_diff_sec > 6 * 3600:\n",
        "            risk_class = 2  # high risk (6-12 hrs)\n",
        "        else:\n",
        "            risk_class = 3  # critical risk (<6 hrs)\n",
        "\n",
        "        X.append(window)\n",
        "        y.append(risk_class)\n",
        "\n",
        "# Convert to numpy arrays and reshape\n",
        "X = np.array(X).reshape(-1, window_size, 1)  # Add channel dimension\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"Generated {len(X)} windows with {len(y)} labels.\")\n",
        "print(\"Class distribution in generated data:\")\n",
        "print(pd.Series(y).value_counts().sort_index())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da8b85af"
      },
      "source": [
        "**Reasoning**:\n",
        "The data has been processed, and the `X` and `y` arrays have been regenerated. The next step is to apply the data augmentation logic to these new `X` and `y` arrays to increase the dataset size and potentially improve class representation before splitting and balancing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cad90ed"
      },
      "source": [
        "def augment_ecg(ecg):\n",
        "    shift = np.random.randint(-fs*5, fs*5)\n",
        "    ecg = np.roll(ecg, shift, axis=0)\n",
        "\n",
        "    scale = np.random.uniform(0.95, 1.05)\n",
        "    ecg = ecg * scale\n",
        "\n",
        "    noise = np.random.normal(0, 0.01, size=ecg.shape)\n",
        "    ecg = ecg + noise\n",
        "\n",
        "    return ecg\n",
        "\n",
        "X_aug, y_aug = [], []\n",
        "\n",
        "for i in range(len(X)):\n",
        "    for _ in range(2):\n",
        "        X_aug.append(augment_ecg(np.copy(X[i])))\n",
        "        y_aug.append(y[i])\n",
        "\n",
        "X = np.concatenate([X, np.array(X_aug)], axis=0)\n",
        "y = np.concatenate([y, np.array(y_aug)], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c29c7a6"
      },
      "source": [
        "**Reasoning**:\n",
        "The data has been augmented, so the next logical step is to re-split the data into training, validation, and test sets to prepare for model training and evaluation, following the previous notebook's structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82d27cdb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.05, stratify=y, random_state=42)\n",
        "val_ratio = 0.15 / 0.95\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_ratio, stratify=y_temp, random_state=42)\n",
        "\n",
        "print(f\"Train size: {X_train.shape[0]}, Validation size: {X_val.shape[0]}, Test size: {X_test.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8840b04"
      },
      "source": [
        "**Reasoning**:\n",
        "The data has been re-split after augmentation. The next logical step, as per the subtask instructions, is to re-execute the SMOTE processing step to balance the classes in the training data (`X_train`, `y_train`) and then print the class distributions. This will also re-apply the NaN/zero check to ensure data integrity for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35656476"
      },
      "source": [
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "\n",
        "valid_mask = (~np.any(np.isnan(X_train_flat), axis=1)) & (np.any(X_train_flat != 0, axis=1))\n",
        "\n",
        "X_train_valid = X_train_flat[valid_mask]\n",
        "y_train_valid = y_train[valid_mask]\n",
        "\n",
        "X_train_invalid = X_train_flat[~valid_mask]\n",
        "y_train_invalid = y_train[~valid_mask]\n",
        "\n",
        "import numpy as np\n",
        "unique, counts = np.unique(y_train_valid, return_counts=True)\n",
        "print(f'Classes in SMOTE-valid data: {dict(zip(unique, counts))}')\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "if len(unique) > 1:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_res_valid, y_res_valid = smote.fit_resample(X_train_valid, y_train_valid)\n",
        "else:\n",
        "    print(\"Warning: Only one class in SMOTE-valid data. Skipping SMOTE.\")\n",
        "    X_res_valid, y_res_valid = X_train_valid, y_train_valid\n",
        "\n",
        "X_res_valid = X_res_valid.reshape(-1, window_size, 1)\n",
        "\n",
        "X_train_clean = np.concatenate([X_res_valid, X_train_invalid.reshape(-1, window_size, 1)], axis=0)\n",
        "y_train_clean = np.concatenate([y_res_valid, y_train_invalid], axis=0)\n",
        "\n",
        "print(f'Final training set after SMOTE and reintegrating invalid windows: {X_train_clean.shape[0]} samples')\n",
        "\n",
        "import pandas as pd\n",
        "print(\"\\nClass distribution in y_train_clean:\")\n",
        "print(pd.Series(y_train_clean).value_counts())\n",
        "\n",
        "print(\"\\nClass distribution in y_val:\")\n",
        "print(pd.Series(y_val).value_counts())\n",
        "\n",
        "print(\"\\nClass distribution in y_test:\")\n",
        "print(pd.Series(y_test).value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6138a89"
      },
      "source": [
        "**Reasoning**:\n",
        "The data has been preprocessed, augmented, split, and re-balanced using SMOTE. The next logical step is to re-train the model with the updated and balanced dataset to see if the 'nan' loss issue has been resolved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c331fbe3"
      },
      "source": [
        "window_size = X_train_clean.shape[1]\n",
        "inputs = Input(shape=(window_size, 1))\n",
        "\n",
        "x = Conv1D(64, 5, activation='relu', padding='same')(inputs)\n",
        "x = MaxPooling1D(pool_size=4)(x)\n",
        "x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "x = MaxPooling1D(pool_size=4)(x)\n",
        "\n",
        "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
        "\n",
        "attn_output = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
        "x = LayerNormalization()(attn_output)\n",
        "\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "y_train_cat = to_categorical(y_train_clean, 4)\n",
        "y_val_cat = to_categorical(y_val, 4)\n",
        "y_test_cat = to_categorical(y_test, 4)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=1000, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(factor=0.5, patience=5),\n",
        "    ModelCheckpoint('sddb_single_lead_best_model.h5', save_best_only=True, monitor='val_loss')\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_clean, y_train_cat,\n",
        "    validation_data=(X_val, y_val_cat),\n",
        "    epochs=500,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a066f3d1"
      },
      "source": [
        "### Debugging NaN Loss: Check for NaNs and Infs in Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "213e4abb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Check X_train_clean for NaNs or Infs\n",
        "if np.any(np.isnan(X_train_clean)) or np.any(np.isinf(X_train_clean)):\n",
        "    print('X_train_clean contains NaN or Inf values!')\n",
        "    # Optionally, print indices or count to investigate further\n",
        "    nan_count = np.sum(np.isnan(X_train_clean))\n",
        "    inf_count = np.sum(np.isinf(X_train_clean))\n",
        "    print(f'NaN count in X_train_clean: {nan_count}')\n",
        "    print(f'Inf count in X_train_clean: {inf_count}')\n",
        "else:\n",
        "    print('X_train_clean is clean (no NaNs or Infs).')\n",
        "\n",
        "# Check y_train_cat for NaNs or Infs (should be one-hot encoded, so only 0s and 1s)\n",
        "if np.any(np.isnan(y_train_cat)) or np.any(np.isinf(y_train_cat)):\n",
        "    print('y_train_cat contains NaN or Inf values!')\n",
        "else:\n",
        "    print('y_train_cat is clean (no NaNs or Infs).')\n",
        "\n",
        "# Also check if y_train_cat is empty or all zeros, which could cause issues\n",
        "if y_train_cat.size == 0:\n",
        "    print('y_train_cat is empty!')\n",
        "elif np.all(y_train_cat == 0):\n",
        "    print('y_train_cat contains all zeros!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cc69d5b"
      },
      "source": [
        "# Task\n",
        "Develop and evaluate a Random Forest Regressor model to predict the risk associated with VF onset using the prepared ECG time series data. This will involve understanding the model's fundamental concepts, preparing the ECG data into features (X) and target (y), training the Random Forest Regressor with appropriate hyperparameters, evaluating its performance using regression metrics such as MAE, MSE, RMSE, and R-squared, making predictions on new data, and interpreting the model's insights, including feature importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62f3e0f3"
      },
      "source": [
        "## Understand Random Forest Regressor Basics\n",
        "\n",
        "### Subtask:\n",
        "Explain the fundamental concepts of a Random Forest Regressor, including how it combines multiple decision trees, the role of bagging (bootstrap aggregating), and how it handles feature importance and generalization to prevent overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "936f4c15"
      },
      "source": [
        "### Understanding Random Forest Regressor Basics\n",
        "\n",
        "A Random Forest Regressor is a powerful **ensemble learning method** that belongs to the supervised learning paradigm. It is used for both classification and regression tasks, but in this context, we focus on regression. Its strength lies in combining the predictions from multiple simple models (decision trees) to produce a more accurate and stable prediction.\n",
        "\n",
        "1.  **Ensemble of Decision Trees**: A Random Forest Regressor operates by constructing a \"forest\" of many individual decision trees during the training phase. Each tree in the forest is trained independently.\n",
        "\n",
        "2.  **Bagging (Bootstrap Aggregating)**: The foundation of Random Forest is a technique called bagging. Instead of using the entire training dataset to train every tree, bagging works as follows:\n",
        "    *   **Bootstrap Sampling**: For each decision tree, a random subset of the original training data is selected with replacement. This means some data points may be sampled multiple times for a single tree, while others may not be sampled at all. This process creates diverse training sets for each tree.\n",
        "    *   **Diverse Trees**: By training each tree on a slightly different subset of the data, the individual trees tend to be somewhat de-correlated, meaning they make errors on different data points.\n",
        "\n",
        "3.  **Feature Randomness**: To further enhance diversity and reduce correlation among trees, the Random Forest introduces another layer of randomness. At each node of a decision tree, when considering a split, the algorithm does not consider all available features. Instead, it randomly selects a subset of features (e.g., `sqrt(total_features)` or `log2(total_features)`) and only considers these selected features for finding the best split at that node. This prevents strong features from dominating all trees and encourages trees to explore different predictive pathways.\n",
        "\n",
        "4.  **Final Prediction by Averaging**: For a regression task, once all the individual decision trees are trained, to make a prediction for a new data point, the input is passed through every tree in the forest. Each tree outputs its own prediction. The final prediction of the Random Forest Regressor is then the **average** of the predictions made by all the individual trees.\n",
        "\n",
        "5.  **Generalization and Overfitting Prevention**: This ensemble approach significantly contributes to better generalization and reduced overfitting compared to a single decision tree. A single decision tree can easily overfit the training data by learning overly complex rules. By averaging the predictions of many diverse and relatively uncorrelated trees, the Random Forest smooths out the individual trees' errors and biases. The randomness introduced by bagging and feature selection ensures that no single tree or feature dominates, leading to a more robust and generalized model.\n",
        "\n",
        "6.  **Feature Importance**: Random Forest Regressors also provide a convenient way to estimate the **feature importance**. This is typically calculated by measuring the average reduction in impurity (e.g., Mean Squared Error for regression) across all splits where a given feature is used, averaged over all trees in the forest. Features that contribute more to reducing impurity are considered more important, helping identify influential factors in the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1cfafa0"
      },
      "source": [
        "### Understanding Random Forest Regressor Basics\n",
        "\n",
        "A Random Forest Regressor is a powerful **ensemble learning method** that belongs to the supervised learning paradigm. It is used for both classification and regression tasks, but in this context, we focus on regression. Its strength lies in combining the predictions from multiple simple models (decision trees) to produce a more accurate and stable prediction.\n",
        "\n",
        "1.  **Ensemble of Decision Trees**: A Random Forest Regressor operates by constructing a \"forest\" of many individual decision trees during the training phase. Each tree in the forest is trained independently.\n",
        "\n",
        "2.  **Bagging (Bootstrap Aggregating)**: The foundation of Random Forest is a technique called bagging. Instead of using the entire training dataset to train every tree, bagging works as follows:\n",
        "    *   **Bootstrap Sampling**: For each decision tree, a random subset of the original training data is selected with replacement. This means some data points may be sampled multiple times for a single tree, while others may not be sampled at all. This process creates diverse training sets for each tree.\n",
        "    *   **Diverse Trees**: By training each tree on a slightly different subset of the data, the individual trees tend to be somewhat de-correlated, meaning they make errors on different data points.\n",
        "\n",
        "3.  **Feature Randomness**: To further enhance diversity and reduce correlation among trees, the Random Forest introduces another layer of randomness. At each node of a decision tree, when considering a split, the algorithm does not consider all available features. Instead, it randomly selects a subset of features (e.g., `sqrt(total_features)` or `log2(total_features)`) and only considers these selected features for finding the best split at that node. This prevents strong features from dominating all trees and encourages trees to explore different predictive pathways.\n",
        "\n",
        "4.  **Final Prediction by Averaging**: For a regression task, once all the individual decision trees are trained, to make a prediction for a new data point, the input is passed through every tree in the forest. Each tree outputs its own prediction. The final prediction of the Random Forest Regressor is then the **average** of the predictions made by all the individual trees.\n",
        "\n",
        "5.  **Generalization and Overfitting Prevention**: This ensemble approach significantly contributes to better generalization and reduced overfitting compared to a single decision tree. A single decision tree can easily overfit the training data by learning overly complex rules. By averaging the predictions of many diverse and relatively uncorrelated trees, the Random Forest smooths out the individual trees' errors and biases. The randomness introduced by bagging and feature selection ensures that no single tree or feature dominates, leading to a more robust and generalized model.\n",
        "\n",
        "6.  **Feature Importance**: Random Forest Regressors also provide a convenient way to estimate the **feature importance**. This is typically calculated by measuring the average reduction in impurity (e.g., Mean Squared Error for regression) across all splits where a given feature is used, averaged over all trees in the forest. Features that contribute more to reducing impurity are considered more important, helping identify influential factors in the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60952a6e"
      },
      "source": [
        "## Prepare Data for Regression\n",
        "\n",
        "### Subtask:\n",
        "Prepare the ECG data by converting categorical risk labels into continuous time-to-VF-onset values and flattening the input features for Random Forest Regressor training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd7b910e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires preparing the ECG data for regression. This involves iterating through the processed signals, generating time windows, calculating the continuous time-to-VF-onset, handling cases with no VF onset, and finally flattening the input features for regression model training and splitting the data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3d7217c"
      },
      "source": [
        "X_reg, y_reg = [], []\n",
        "\n",
        "for file_num, signal in ecq_signals_dict.items():\n",
        "    vf_onset_sample = vf_onset_times_in_samples.get(file_num)\n",
        "\n",
        "    if len(signal) < window_size:\n",
        "        print(f\"Skipping file {file_num} due to insufficient signal length for regression.\")\n",
        "        continue\n",
        "\n",
        "    for start in range(0, len(signal) - window_size + 1, stride):\n",
        "        window = signal[start:start + window_size]\n",
        "        window_center = start + window_size // 2\n",
        "\n",
        "        if vf_onset_sample is None:\n",
        "            # If no VF onset, assign a large positive time as 'very low risk'\n",
        "            time_diff_sec = 48 * 3600  # 48 hours in seconds\n",
        "        else:\n",
        "            # Calculate time difference from window center to VF onset in seconds\n",
        "            time_diff_sec = (vf_onset_sample - window_center) / fs\n",
        "\n",
        "            # Skip windows that are after the VF onset\n",
        "            if time_diff_sec < 0:\n",
        "                continue\n",
        "\n",
        "        X_reg.append(window)\n",
        "        y_reg.append(time_diff_sec)\n",
        "\n",
        "X_reg = np.array(X_reg)\n",
        "y_reg = np.array(y_reg)\n",
        "\n",
        "# Reshape X_reg to 2D for scikit-learn regressors\n",
        "X_reg_flattened = X_reg.reshape(X_reg.shape[0], -1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg_flattened, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"X_train_reg shape: {X_train_reg.shape}\")\n",
        "print(f\"X_test_reg shape: {X_test_reg.shape}\")\n",
        "print(f\"y_train_reg shape: {y_train_reg.shape}\")\n",
        "print(f\"y_test_reg shape: {y_test_reg.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be0b0771"
      },
      "source": [
        "## Train the Random Forest Regressor Model\n",
        "\n",
        "### Subtask:\n",
        "Train a Random Forest Regressor model using the prepared training data (`X_train_reg`, `y_train_reg`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6552e280"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires training a Random Forest Regressor model. This involves importing the necessary class, instantiating the model with hyperparameters, and then fitting it to the prepared training data. I will include these steps in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f021ce30"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Instantiate the Random Forest Regressor model\n",
        "# Using n_estimators=100 and random_state=42 for reproducibility\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "print(\"Random Forest Regressor model trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd057194"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained Random Forest Regressor model's performance on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34bcdc91"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires evaluating the trained Random Forest Regressor model's performance on the test data. This involves making predictions, calculating MAE, MSE, RMSE, and R-squared, and printing the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1b8bafb"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_reg = rf_regressor.predict(X_test_reg)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python (.venv)",
      "name": ".venv"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}